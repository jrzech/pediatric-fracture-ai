{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vM54r6jlKTII"
   },
   "source": [
    "#derived from code shared by Meta AI. Many thanks to the Meta AI team for developing and maintaining this incredible Detectron2 library (and PyTorch itself!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FsePPpwZSmqt"
   },
   "outputs": [],
   "source": [
    "!python -m pip install pyyaml==5.1\n",
    "import sys, os, distutils.core\n",
    "import torch, detectron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZyAvNCJMmvFF"
   },
   "outputs": [],
   "source": [
    "!pip install opencv-python\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "#from google.colab.patches import cv2_imshow\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cv2_imshow(im):\n",
    "    image = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HUjkwRsOn1O0",
    "outputId": "46bafc09-d3d5-482b-90b6-315b75040f8e"
   },
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n",
    "predictor = DefaultPredictor(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PIbAM2pv-urF"
   },
   "outputs": [],
   "source": [
    "label_to_idx={'Femur':0,'Tibia':1}\n",
    "from detectron2.structures import BoxMode\n",
    "\n",
    "def get_leg_dicts(csv_path, fold):\n",
    "    file=pd.read_csv(csv_path)\n",
    "    file=file[file['fold']==fold]\n",
    "    \n",
    "    dataset_dicts = []\n",
    "    unique_files = file['file'].unique()\n",
    "    \n",
    "    fileidx=0\n",
    "    for thisfile in unique_files:\n",
    "        temp = file[file['file']==thisfile]\n",
    "        \n",
    "        record={}\n",
    "        record[\"file_name\"] = \"/home/appuser/detectron2_repo/SHARED/image/eos-image/\" + thisfile\n",
    "        record[\"image_id\"] = fileidx\n",
    "        record[\"height\"] = temp['height'].iloc[0]\n",
    "        record[\"width\"] = temp['width'].iloc[0]\n",
    "        \n",
    "        objs = []\n",
    "      \n",
    "        for idx in range(0,len(temp)):\n",
    "            poly=temp['points'].iloc[idx].replace(\"[\",\"\").replace(\"]\",\"\").split(\",\")\n",
    "            poly = [float(x) for x in poly]\n",
    "            px=poly[0::2]\n",
    "            py=poly[1::2]\n",
    "\n",
    "            obj = {\n",
    "                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
    "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "                \"segmentation\": [poly],\n",
    "                \"category_id\": label_to_idx[temp['label'].iloc[idx]]\n",
    "            }\n",
    "            objs.append(obj)\n",
    "        record[\"annotations\"] = objs\n",
    "        dataset_dicts.append(record)\n",
    "        fileidx=fileidx+1\n",
    "    return dataset_dicts\n",
    "\n",
    "for d in [\"train\",\"tune\",\"test\"]:\n",
    "    DatasetCatalog.register(\"leg_\" + d, lambda d=d: get_leg_dicts(\"eoscsv.csv\",d))\n",
    "    MetadataCatalog.get(\"leg_\" + d).set(thing_classes=[\"Femur\",\"Tibia\"])\n",
    "    \n",
    "leg_metadata = MetadataCatalog.get(\"leg_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "from detectron2.engine.hooks import HookBase\n",
    "from detectron2.evaluation import inference_context\n",
    "from detectron2.utils.logger import log_every_n_seconds\n",
    "from detectron2.data import DatasetMapper, build_detection_test_loader\n",
    "import detectron2.utils.comm as comm\n",
    "import torch\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "class LossEvalHook(HookBase):\n",
    "    def __init__(self, eval_period, model, data_loader):\n",
    "        self._model = model\n",
    "        self._period = eval_period\n",
    "        self._data_loader = data_loader\n",
    "    \n",
    "    def _do_loss_eval(self):\n",
    "        # Copying inference_on_dataset from evaluator.py\n",
    "        total = len(self._data_loader)\n",
    "        num_warmup = min(5, total - 1)\n",
    "            \n",
    "        start_time = time.perf_counter()\n",
    "        total_compute_time = 0\n",
    "        losses = []\n",
    "        for idx, inputs in enumerate(self._data_loader):            \n",
    "            if idx == num_warmup:\n",
    "                start_time = time.perf_counter()\n",
    "                total_compute_time = 0\n",
    "            start_compute_time = time.perf_counter()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            total_compute_time += time.perf_counter() - start_compute_time\n",
    "            iters_after_start = idx + 1 - num_warmup * int(idx >= num_warmup)\n",
    "            seconds_per_img = total_compute_time / iters_after_start\n",
    "            if idx >= num_warmup * 2 or seconds_per_img > 5:\n",
    "                total_seconds_per_img = (time.perf_counter() - start_time) / iters_after_start\n",
    "                eta = datetime.timedelta(seconds=int(total_seconds_per_img * (total - idx - 1)))\n",
    "                log_every_n_seconds(\n",
    "                    logging.INFO,\n",
    "                    \"Loss on Validation  done {}/{}. {:.4f} s / img. ETA={}\".format(\n",
    "                        idx + 1, total, seconds_per_img, str(eta)\n",
    "                    ),\n",
    "                    n=5,\n",
    "                )\n",
    "            loss_batch = self._get_loss(inputs)\n",
    "            losses.append(loss_batch)\n",
    "        mean_loss = np.mean(losses)\n",
    "        self.trainer.storage.put_scalar('validation_loss', mean_loss)\n",
    "        comm.synchronize()\n",
    "\n",
    "        return losses\n",
    "            \n",
    "    def _get_loss(self, data):\n",
    "        # How loss is calculated on train_loop \n",
    "        metrics_dict = self._model(data)\n",
    "        metrics_dict = {\n",
    "            k: v.detach().cpu().item() if isinstance(v, torch.Tensor) else float(v)\n",
    "            for k, v in metrics_dict.items()\n",
    "        }\n",
    "        total_losses_reduced = sum(loss for loss in metrics_dict.values())\n",
    "        return total_losses_reduced\n",
    "        \n",
    "        \n",
    "    def after_step(self):\n",
    "        next_iter = self.trainer.iter + 1\n",
    "        is_final = next_iter == self.trainer.max_iter\n",
    "        if is_final or (self._period > 0 and next_iter % self._period == 0):\n",
    "            self._do_loss_eval()\n",
    "        self.trainer.storage.put_scalars(timetest=12)\n",
    "        \n",
    "class MyTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "        return COCOEvaluator(dataset_name, cfg, True, output_folder)\n",
    "                     \n",
    "    def build_hooks(self):\n",
    "        hooks = super().build_hooks()\n",
    "        hooks.insert(-1,LossEvalHook(\n",
    "            cfg.TEST.EVAL_PERIOD,\n",
    "            self.model,\n",
    "            build_detection_test_loader(\n",
    "                self.cfg,\n",
    "                self.cfg.DATASETS.TEST[0],\n",
    "                DatasetMapper(self.cfg,True)\n",
    "            )\n",
    "        ))\n",
    "        return hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7unkuuiqLdqd",
    "outputId": "ba1716cd-3f3b-401d-bae5-8fbbd2199d9c"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"leg_train\",)\n",
    "cfg.DATASETS.TEST = (\"leg_tune\",)\n",
    "cfg.TEST.EVAL_PERIOD =2000\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD=2000\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml\")\n",
    "cfg.SOLVER.IMS_PER_BATCH = 1  \n",
    "cfg.SOLVER.BASE_LR = 0.00025  \n",
    "cfg.SOLVER.MAX_ITER = 100000    \n",
    "cfg.SOLVER.STEPS = []       \n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512   \n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2 \n",
    "cfg.INPUT.MIN_SIZE_TRAIN= (416,448,480,512,544,576,608)\n",
    "cfg.INPUT.MAX_SIZE_TRAIN= 2560\n",
    "\n",
    "cfg.INPUT.MIN_SIZE_TEST= 512\n",
    "cfg.INPUT.MAX_SIZE_TEST= 2560\n",
    "\n",
    "cfg.INPUT.RANDOM_FLIP=\"horizontal\"\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = MyTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
